# Chapter 2: Madison Planning
 
## Build Like a Product Manager
 
## What You'll Learn
 
By the end of this chapter, you will:
- Write a Product Requirements Document (PRD) like tech companies use
- Identify the gap between your current skills and your target job
- Design a multi-agent AI architecture
- Define realistic scope for your Madison project
- Conduct effective peer reviews that make everyone better
 
**Time Investment**: 8-10 hours 
**Deliverable**: A comprehensive PRD on Figma that anyone could build from
 
## The 80/20 Philosophy in Action
 
**The Reality**: In the real world, 80% of success is shipping something that works.
 
**The Other 20%**: Polish, insight, and that extra mile—that's what beats your competition.
 
This week, you're learning to plan like a professional product manager. Not because planning is fun (it's not), but because:
 
1. **Good planning = faster building**
2. **Clear requirements = less wasted time**
3. **PRDs get you hired** (seriously, recruiters love seeing this skill)
 
## Why This Chapter Matters
 
Most students jump straight into coding. They:
- Build the wrong thing
- Realize halfway through it won't work
- Have no clear success criteria
- Can't explain their decisions
- End up with nothing to show
 
**The professionals?** They spend time planning. They:
- Know exactly what they're building and why
- Can defend their technical decisions
- Have clear milestones and success metrics
- Ship on time because they planned for reality
- Have documentation that impresses employers
 
**This chapter makes you a professional.**
 
## Part 1: Pick Your Lane (Dream Job Analysis)
 
### The Challenge
 
Find ONE real job posting you could apply for in 6-12 months. Not 5 years from now. Not "someday." A real job you could actually get with the skills you'll have by summer.
 
### Why This Matters
 
Everything you build this semester should point toward a specific role. Not "working in AI" (too vague). A specific position at a specific type of company.
 
**Without a target**:
- You build random things
- Your portfolio lacks focus
- You can't tell a coherent story
- Recruiters don't know where you fit
 
**With a target**:
- Every project proves a relevant skill
- Your portfolio tells a clear story
- You can explain exactly why you're qualified
- Recruiters immediately see the fit
 
### Finding Your Target Job
 
**Where to Look**:
- **LinkedIn Jobs**: Best for established companies
- **Indeed**: Broad coverage, good for comparing requirements
- **AngelList**: Startups and growth-stage companies
- **Y Combinator Jobs**: High-growth startups
- **Company Career Pages**: Direct from source
 
**What to Look For**:
 
**1. Timeline Check**
Can you realistically qualify in 6-12 months?
 
❌ **Too Senior**: "5+ years experience, managed teams of 10+"   
✅ **Right Level**: "0-2 years experience" or "New grad role" or "Early career"
 
**2. Skill Alignment**
Do you have 50%+ of the required skills?
 
❌ **Too Far**: You have 1 out of 10 requirements   
✅ **Achievable Stretch**: You have 5-6 out of 10, can learn the rest
 
**3. Interest Check**
Would you actually want this job?
 
❌ **Purely Strategic**: Boring work you'd hate   
✅ **Genuinely Interesting**: Role you'd be excited to do
 
**4. Madison Connection**
Can your Madison project address a need in this role?
 
❌ **No Connection**: Role has nothing to do with your project   
✅ **Clear Connection**: Your Madison project solves problems this role faces
 
### Your Deliverable
 
Post on your Figma board:
 
**1. Company + Exact Title**  
Be specific. "Marketing Automation Engineer at HubSpot" not "marketing job"
 
**Why**: Specific = you've done your research. Vague = you're guessing.
 
**2. Link or Screenshot**  
Actual job posting. Prove it's real.
 
**Why**: Forces you to work with real requirements, not imagined ones.
 
**3. Top 3 Technical Requirements**  
Copy them exactly from the posting.
 
**Examples**:
- "Experience with marketing automation tools (Zapier, n8n, or similar)"
- "Proven ability to work with APIs and data integration"
- "Python or JavaScript for building marketing tools"
 
**Why**: These are your gap analysis starting point.
 
**4. One Sentence: Why This Role?**  
What makes you interested?
 
❌ **Generic**: "It's a good company and the role sounds interesting"   
✅ **Specific**: "I want to build AI-powered marketing automation tools that help teams scale without hiring, which is exactly what this role does"
 
### Excellence Level (Bonus Points)
 
**Find the Hiring Manager on LinkedIn**
 
**How**:
1. Find the company's page
2. Search "[Role] at [Company]" on LinkedIn
3. Look at who's in that department
4. Find who's likely hiring (Director/VP level)
 
**Learn About Them**:
- Their background
- Their recent posts/articles
- Their team's initiatives
- What they care about
 
**Why This Matters**:
- Shows initiative beyond basic requirements
- Helps you understand what they're really looking for
- Gives you talking points for networking/interviews
- Demonstrates research skills
 
**Example**:
"Hiring manager is Sarah Chen, VP of Marketing Operations at Acme. She previously led marketing ops at two other SaaS companies, recently posted about the challenge of scaling content production without losing quality, and her team just launched a new workflow automation initiative. My Madison project—a multi-platform content adapter—directly addresses the scaling problem she's discussing."
 
## Part 2: Gap Analysis - What's Between You and That Job?
 
### The Uncomfortable Truth
 
You're not qualified for that job. Yet.
 
**That's okay.** The gap between where you are and where you want to be? That's your curriculum for the semester.
 
### Creating Your Gap Analysis Table
 
**The Format**:
 
| They Want | I Have | Gap to Fill | Madison Could Help By... |
|-----------|---------|-------------|--------------------------|
| [Requirement] | [Current level] | [What's missing] | [How your project addresses it] |
 
### How to Fill It Out
 
**Column 1: They Want**  
Copy directly from the job posting. Exact words matter.
 
**Column 2: I Have**  
Be honest. What do you actually know?
 
**Rating Scale**:
- **None**: Never used it
- **Basic**: Used it, understand fundamentals
- **Intermediate**: Can work independently
- **Advanced**: Can solve complex problems
 
**Column 3: Gap to Fill**  
What's the specific gap?
 
Not: "Need to learn automation" 
But: "Need hands-on experience building and deploying multi-step workflows"
 
**Column 4: Madison Could Help By...**  
How your project fills this gap.
 
Not: "I'll learn it" 
But: "Building n8n workflows that scrape competitor social posts, analyze trends, and send automated alerts"
 
### Example Gap Analysis
 
**Job Posting**: Marketing Automation Engineer at HubSpot
 
| They Want | I Have | Gap to Fill | Madison Could Help By... |
|-----------|---------|-------------|--------------------------|
| Experience with workflow automation tools (Zapier, n8n, Workato) | None | Need hands-on experience building and deploying workflows | Building multi-step n8n workflows for competitor monitoring automation, deploying to production |
| Understanding of marketing technology stack | Basic | Need practical knowledge of how tools integrate | Integrating Madison with real marketing tools (social APIs, analytics platforms, content management) |
| Python or JavaScript for automation | Intermediate Python | Need production deployment experience | Writing Python agents for Madison, deploying to cloud, documenting for open-source contribution |
| API integration experience | Basic understanding | Need real examples of RESTful API work | Connecting Madison to 5+ marketing platform APIs, handling auth, rate limits, and error cases |
| Data analysis for marketing insights | Basic Excel/SQL | Need programmatic analysis skills | Building automated trend detection and reporting systems using Python data analysis libraries |
 
### Example Gap Analysis (Different Role)
 
**Job Posting**: Content Marketing Specialist with AI Skills
 
| They Want | I Have | Gap to Fill | Madison Could Help By... |
|-----------|---------|-------------|--------------------------|
| Experience using AI tools for content creation | Basic ChatGPT use | Need systematic content generation workflows | Building multi-platform content adapter that demonstrates sophisticated prompt engineering and automation |
| Understanding of SEO and content optimization | Basic knowledge | Need practical optimization experience | Creating SEO content optimizer that analyzes and improves content programmatically |
| Social media management across platforms | Personal use only | Need professional multi-platform experience | Building social media scheduling and performance tracking system |
| Data-driven content strategy | None | Need analytics skills | Building content performance analyzer that tracks what works and suggests topics |
 
### Excellence Level (Research Their Tech Stack)
 
**Go Deeper**:
1. Check their engineering blog
2. Look at their job postings for engineers
3. Search "[Company] tech stack" on Google
4. Check StackShare or similar sites
5. Look at what their engineers discuss on Twitter/LinkedIn
 
**Document**:
- What specific tools they use
- What frameworks they prefer
- What architectural patterns they follow
- What they're experimenting with
 
**Why**:
- Shows initiative and research skills
- Helps you make better architecture decisions
- Gives you specific things to learn
- Provides conversation topics for interviews
 
**Example**:
"HubSpot's engineering blog shows they use TypeScript + React for frontend, Python microservices on AWS for backend, and extensively use event-driven architecture. They're experimenting with AI agents for marketing automation. My Madison project will use Python microservices and experiment with multi-agent systems—directly aligned with their tech direction."
 
## Part 3: Your PRD (Product Requirements Document)
 
### What Is a PRD?
 
**Simple Answer**: A document that explains what you're building and why.
 
**Real Answer**: The document that prevents you from building the wrong thing.
 
**Why It Matters**:
- Every tech company uses PRDs
- Product managers write them constantly
- "Can you write a PRD?" is literally a job interview question
- Having PRD examples in your portfolio = instant credibility
 
### The Four Sections
 
Every PRD has the same basic structure. Master this, and you can write PRDs for any project.
 
---
 
### Section 1: Problem Statement (10 points)
 
**The Goal**: Convince someone the problem is worth solving.
 
**The Three Questions**:
 
**1. What specific problem does your target company/role face?**
 
Not: "Marketing is hard"   
But: "Marketing teams at B2B SaaS companies spend 10-15 hours per week manually monitoring competitor social accounts to identify trending topics and competitive moves"
 
**Why Specific Matters**:
- Shows you understand the actual problem
- Gives you measurable success criteria
- Makes it clear who benefits
 
**2. Who experiences this problem?**
 
Not: "Companies"   
But: "Social media managers at Series B+ B2B SaaS companies competing in crowded markets where being first to trending topics matters"
 
**Why This Matters**:
- Defines your target user
- Helps you make better design decisions
- Shows you understand the market
 
**3. What's the cost of not solving it?**
 
Quantify the pain. Use numbers.
 
Examples:
- **Time**: "10-15 hours per week = $25,000-40,000 per year in labor costs"
- **Opportunity**: "Missing trending topics means missed engagement opportunities worth $X"
- **Competitive**: "Competitors respond to industry news faster, appearing more thought-leadership positioned"
 
**Why Cost Matters**:
- Proves it's worth building
- Gives you ROI calculations
- Shows business thinking (not just technical)
 
**Good Problem Statement Examples**:
 
**Example 1: Social Media Monitoring**
 
"Marketing teams at growth-stage B2B SaaS companies (Series B, 50-200 employees) struggle to monitor competitors effectively. Social media managers spend 10-15 hours per week manually checking 5-10 competitor accounts across Twitter, LinkedIn, and industry forums to identify competitive moves, trending topics, and messaging changes.
 
At $60-80K salaries, that's $15,000-25,000 annually in labor costs just for monitoring. More critically, manual monitoring means delays—by the time they see a competitor's viral post about an industry trend, it's too late to join the conversation. Missing these moments costs thought leadership positioning and engagement opportunities.
 
The real cost: competitors who automate this appear more responsive, more informed, and more industry-leading—all because they see and react faster."
 
**Example 2: Content Repurposing**
 
"Content marketing teams at B2B companies face a distribution efficiency problem. After spending 8-10 hours creating a high-quality blog post, they need to adapt it for 5-6 different platforms (Twitter thread, LinkedIn post, email newsletter excerpt, Instagram caption, Facebook post). Each adaptation takes 20-30 minutes and requires understanding platform-specific best practices.
 
For a team publishing 4 blog posts per month, that's 10-12 hours monthly spent on manual reformatting—120-144 hours annually at $40-50/hour = $4,800-7,200 in labor costs. More importantly, many teams skip some platforms because reformatting is tedious, leaving distribution opportunities on the table.
 
The cost: great content reaches 1-2 platforms instead of 6, reducing total potential reach by 60-70%."
 
**Example 3: Synthetic Personas**
 
"Marketing teams at startups and small businesses make critical targeting decisions based on assumptions rather than data. Creating research-based customer personas requires customer interviews (20-30 interviews), survey analysis (100+ responses), and synthesis time (15-20 hours per persona).
 
At $150-200/hour consulting rates, persona development costs $3,000-6,000 per set. Most companies under $5M revenue skip this entirely, targeting 'everyone' or guessing at their ideal customer based on founders' intuitions.
 
The cost: wasted ad spend targeting wrong audiences, messaging that doesn't resonate, product features nobody wants. Per CB Insights, 42% of startup failures cite 'no market need'—often because they never understood their actual customer."
 
**Why These Work**:
- Specific company type and size
- Specific people affected
- Quantified time and costs
- Business impact clearly stated
- Real consequences of not solving
 
---
 
### Section 2: Proposed Solution (10 points)
 
**The Goal**: Explain your approach clearly enough that anyone could build it.
 
**The Three Questions**:
 
**1. Your Madison-Powered Approach**
 
Explain what you're building, specifically.
 
**Social Media Monitor Example**:
"A multi-agent Madison system that: (1) automatically fetches competitor posts from Twitter and LinkedIn every 6 hours via APIs, (2) analyzes posts for trending industry keywords and engagement spikes, (3) identifies content that's outperforming typical engagement by 2x+, and (4) sends daily digest emails with top findings and suggested response opportunities"
 
**Content Adapter Example**:
"A Madison workflow that: (1) takes a blog post URL, (2) extracts key points and main message, (3) uses GPT-4 to generate platform-optimized versions (Twitter thread with hook, LinkedIn post with professional tone, Instagram caption with hashtags), (4) outputs all versions in one click, maintaining core message while adapting to platform best practices"
 
**Synthetic Persona Example**:
"An AI-powered persona generator that: (1) ingests customer data from surveys and interviews (text responses), (2) uses NLP to identify common patterns in goals, pain points, and behaviors, (3) clusters customers into 3-5 distinct segments, (4) generates detailed persona documents with demographics, motivations, preferred channels, and buying triggers for each segment"
 
**Structure Your Solution**:
What it does:
Input: [What data it needs]
Processing: [How it analyzes]
Output: [What it produces]
Integration: [How it fits into existing workflows]
 
**2. Why Madison vs. Other Solutions?**
 
What makes your approach better?
 
**Compare to Alternatives**:
 
**For Monitoring Tools**:
- Manual checking: Slow, inconsistent, humans miss things
- Social listening tools (Hootsuite, Sprout): Expensive ($100-300/month), designed for own brand not competitors
- Google Alerts: Misses social media, too general
- Custom-built: Expensive to build and maintain
 
**For Content Tools**:
- Manual reformatting: Slow, tedious, teams skip platforms
- Content calendars (CoSchedule): Schedule but don't reformat
- AI writing tools (Jasper, Copy.ai): Generic output, no platform optimization
- Copy-paste: Inconsistent, platform best practices ignored
 
**Your Madison Advantage**:
- Madison's agent architecture allows specialized processing per platform
- Open-source = customizable for your specific needs
- n8n integration = works with existing marketing stack
- Extensible = add new platforms or features easily
- Free (except AI API costs)
 
**Example**:
"Madison's multi-agent architecture is ideal for competitor monitoring because unlike monolithic social listening tools, we can deploy specialized agents: one for Twitter tracking, one for LinkedIn, one for trend detection, one for engagement analysis. Each agent can be optimized for its specific platform's API and data structure. Plus, the n8n integration means marketers can customize alerts, filters, and outputs without writing code."
 
**3. What Makes This Technically Interesting?**
 
Show you understand the technical challenge.
 
**Discuss**:
- What's hard about this problem
- What technical decisions you'll make
- What trade-offs exist
- Why your approach handles these challenges
 
**Competitor Monitor Example**:
"The technical challenge is signal vs. noise. Competitors post 10-50 times per day; most posts are routine, but 1-2 might be strategically important (product launch, pricing change, major campaign). My approach uses multi-dimensional scoring: (1) engagement anomaly detection (2x+ normal engagement), (2) keyword matching against strategic terms ('pricing,' 'launch,' 'feature'), (3) sentiment shifts (suddenly negative = possible crisis), (4) posting pattern changes (sudden frequency = campaign). Only posts scoring high on 2+ dimensions get flagged as 'high-priority.' This reduces noise by 90%+ while catching strategic signals."
 
**Content Adapter Example**:
"The challenge is maintaining message fidelity while optimizing for platform. Simply shortening for Twitter loses context; simply copy-pasting to LinkedIn ignores professional norms. My approach uses a two-step process: (1) Extract Agent identifies core message and 3-5 key points, (2) Adaptation Agent rewrites for each platform using platform-specific prompts (Twitter: hook-first, thread structure, character limits; LinkedIn: professional tone, business context, longer-form). This preserves meaning while respecting platform cultures."
 
**Synthetic Persona Example**:
"The challenge is going from unstructured qualitative data (interview transcripts, survey responses) to structured personas. My approach uses: (1) NLP to extract goals, pain points, and behavioral patterns from text responses, (2) clustering algorithm to group similar customers, (3) statistical profiling to identify defining characteristics per cluster, (4) GPT-4 to synthesize findings into readable persona documents. The multi-step approach ensures personas are data-driven (not assumed) while remaining human-readable."
 
---
 
### Section 3: User Stories (10 points)
 
**What Are User Stories?**
 
A format for describing features from the user's perspective.
 
**The Format**:
"As a [role], I want [feature] so that [benefit]"
 
**Why This Format Works**:
- Forces you to think from user perspective
- Connects features to actual needs
- Industry-standard format
- Shows you understand user-centered design
 
**Write Three User Stories**
 
**Good Examples - Competitor Monitor**:
 
**Story 1 - Core Use Case**:  
"As a social media manager at a B2B SaaS company, I want to receive a daily digest of my competitors' highest-performing posts from the past 24 hours so that I can identify trending topics to respond to without spending 90 minutes manually checking their accounts."
 
**Story 2 - Alert Use Case**:  
"As a marketing director, I want to receive immediate Slack alerts when competitors post about pricing, product launches, or major campaigns so that I can brief my team and prepare our response within hours, not days."
 
**Story 3 - Analysis Use Case**:  
"As a content strategist, I want to see which topics competitors are posting about most frequently so that I can identify content gaps where we're under-represented in industry conversations."
 
---
 
**Good Examples - Content Adapter**:
 
**Story 1 - Core Use Case**:  
"As a content marketer, I want to paste a blog post URL and receive platform-optimized versions for Twitter, LinkedIn, and Instagram in one click so that I can distribute content across all channels without spending 30-45 minutes reformatting."
 
**Story 2 - Quality Use Case**:  
"As a marketing manager, I want the adapted content to maintain our core message while following each platform's best practices so that our content performs well everywhere without me reviewing and editing each version."
 
**Story 3 - Efficiency Use Case**:  
"As a small business owner with limited time, I want to write once and distribute everywhere so that I can maintain presence on 5 platforms without hiring a social media manager."
 
---
 
**Good Examples - Synthetic Personas**:
 
**Story 1 - Core Use Case**:  
"As a marketing strategist, I want to upload 50 customer interview transcripts and receive 3-4 data-driven persona documents so that I can base our targeting on actual customer patterns rather than assumptions, without spending 20 hours manually synthesizing."
 
**Story 2 - Segmentation Use Case**:  
"As a product manager, I want to see how our customers cluster into distinct groups with different goals and pain points so that I can prioritize features that matter most to our largest segments."
 
**Story 3 - Validation Use Case**:  
"As a founder, I want to validate or challenge my assumptions about our ideal customer by seeing what patterns actually emerge from customer data so that I stop wasting ad budget on audiences that don't convert."
 
---
 
**Bad Examples (And Why)**:
 
❌ "As a user, I want the tool to work so that I can use it"
- Too vague
- No specific role
- No specific feature
- No measurable benefit
 
❌ "As a developer, I want to build an API so that it's scalable"
- Wrong perspective (you're not the end user)
- Technical focus, not user benefit
- No user problem being solved
 
---
 
### Section 4: Success Metrics (10 points)
 
**The Goal**: Define how you'll know if you succeeded.
 
**The Three Questions**:
 
**1. What Improves? By How Much?**
 
Define clear, measurable improvements.
 
**Good Metrics by Project Type**:
 
**For Monitoring/Scraping Tools**:
- Time saved: "Reduce monitoring time from 10 hours/week to 30 minutes/week"
- Coverage: "Track 10 competitors across 3 platforms automatically"
- Speed: "Detect trending topics within 2 hours vs. 24 hours manual"
 
**For Content Tools**:
- Time saved: "Reduce content adaptation from 45 minutes to 2 minutes per post"
- Quality: "Maintain 90%+ message fidelity across platforms"
- Reach: "Enable posting to 5 platforms instead of 2"
 
**For Analysis Tools**:
- Accuracy: "Identify customer segments with 85%+ confidence"
- Speed: "Process 100 survey responses in 5 minutes vs. 20 hours manual"
- Insight: "Generate 5 actionable insights per analysis"
 
**Bad Metrics**:
- "Better marketing" (not measurable)
- "Happier teams" (subjective)
- "More efficient" (vague)
 
**2. Time Saved? Errors Reduced? Cost Cut?**
 
Quantify the business impact.
 
**Framework**:  
Current State:  
Process: [How it works now]  
Time: [How long it takes]  
Cost: [What it costs]   
Error Rate: [How often problems occur]  

Future State (with your solution):  
Process: [How it will work]  
Time: [New time required]  
Cost: [New cost]  
Error Rate: [New error rate]  

Improvement:  
Time: [X% faster or X hours saved]  
Cost: [Y% cheaper or $Y saved]  
Quality: [Z% fewer errors]
 
**Competitor Monitor Example**:  
Current State:  
Social media manager manually checks 8 competitors daily  
15-20 minutes per competitor = 2-3 hours daily  
At $60K salary = $7,500-11,000 annually just on monitoring  
Misses posts published outside work hours (60% of content)  

With Madison Competitor Monitor:  
Automated checking every 6 hours  
10-15 minutes daily reviewing digest of high-priority posts only  
At $60K salary = $1,000-1,500 annually  
Catches 100% of posts 24/7  

Improvement:  
Time: 90% reduction (2.5 hours saved daily)  
Cost: $6,000-10,000 saved annually  
Coverage: 2.5x more competitive intelligence gathered
 
**Content Adapter Example**:  
Current State:  
Content marketer manually reformats blog post for 5 platforms  
30-45 minutes total adaptation time  
Creates 4 blog posts monthly = 2-3 hours monthly reformatting  
Often skips 1-2 platforms due to time constraints

With Madison Content Adapter:  
Automated platform-specific adaptation  
5 minutes reviewing/editing outputs  
4 blog posts monthly = 20 minutes monthly  
All platforms covered consistently  

Improvement:  
Time: 85% reduction (2+ hours saved monthly)  
Reach: 40% increase (all platforms covered vs. 3-4)  
Consistency: All platforms get timely content
 
**3. How Would You Measure Success?**
 
Define your testing methodology.
 
**Approaches**:
 
**Quantitative Testing**:
- "Test with 200 competitor posts, measure detection accuracy for trending topics"
- "Measure processing speed with datasets of 100, 500, and 1000 posts"
- "Track precision (flagged posts that were actually important) and recall (important posts that were flagged)"
 
**User Testing**:
- "Have 3 social media managers use it for one week, measure time spent on competitor monitoring"
- "Survey satisfaction on 1-5 scale across 5 dimensions"
- "Track adoption: are they still using it after 30 days?"
 
**Business Impact**:
- "Monitor response time to trending topics before/after implementation"
- "Measure competitive intelligence quality (insights gathered per week)"
- "Track cost per competitive insight"
 
**Example - Competitor Monitor**:  
"I'll measure success through three methods:
 
1. **Accuracy Testing**: Run the system for 2 weeks alongside manual monitoring. Count how many strategically important posts it correctly flagged. Success = 85%+ precision (flagged posts were actually important) and 90%+ recall (caught all important posts).
 
2. **Speed Testing**: Measure time from competitor post to alert received. Success = alerts within 2 hours for high-priority posts.
 
3. **User Testing**: Have two social media managers use it for one week. Success = 75%+ time reduction on competitor monitoring and 4+/5 satisfaction rating."
 
**Example - Content Adapter**:  
"I'll measure success through three methods:
 
1. **Quality Testing**: Have 3 content marketers rate platform-adapted versions on message fidelity (1-5). Success = 4+ average rating.
 
2. **Speed Testing**: Time the full workflow (blog URL → 5 platform versions). Success = under 3 minutes including review.
 
3. **Adoption Testing**: Two content marketers use it for all blog posts over 2 weeks. Success = they adopt it for at least 80% of posts and rate it 4+/5 on usefulness."
 
---
 
### Excellence Level for Your PRD
 
**How to Get Excellence Points** (8 additional points possible):
 
**1. Industry Research (2 pts)**
- Cite actual studies or reports
- Reference market data
- Show competitive analysis
- Quote industry experts
 
Examples:
- "According to HubSpot's 2024 State of Marketing report, 63% of marketers say competitor intelligence is critical but 71% do it manually"
- "Per Sprout Social's research, brands that respond to trends within 2 hours see 3x engagement vs. those responding after 24 hours"
 
**2. Specific API/Tool References (2 pts)**
- Name specific APIs you'll integrate
- Reference specific n8n nodes
- List specific AI models you'll use
- Show technical research
 
Example: "Will integrate with: Twitter API v2 (tweet timeline endpoint), LinkedIn Marketing API (company posts), using n8n's HTTP Request, JSON Parse, and Switch nodes, powered by GPT-4 for trend analysis and GPT-3.5 for summarization (cost optimization)."
 
**3. Edge Cases and Failure Modes (2 pts)**
- What could go wrong?
- How will you handle errors?
- What are system limitations?
- What won't it do well?
 
Examples:
- "Edge cases: API rate limits hit if checking too frequently (will throttle to every 6 hours), sarcastic posts may be misclassified as negative (requires human review flags), non-English posts require translation API (out of scope for MVP)"
- "Failure modes: If API goes down, queue requests and retry; if GPT-4 is slow, fall back to GPT-3.5; if trend detection has high false positives, add confidence thresholds"
 
**4. Professional Format (2 pts)**
- Clean, organized layout
- Consistent formatting
- Visual hierarchy
- Professional presentation
- Proper headers/sections
- No spelling/grammar errors
 
## Part 4: Technical Architecture (30 points)
 
### The Madison Agent Design (15 points)
 
**What Are AI Agents?**
 
An AI agent is a program that:
1. Takes input
2. Makes decisions (usually using AI)
3. Produces output
4. Can communicate with other agents
 
**In Madison**: You'll have multiple agents working together, each with a specific job.
 
### Designing Your Agents
 
**Think Like a Team Manager**:
 
If you had a team of specialists, what would each person do?
 
**Example Project 1: Competitor Social Monitor**
 
**Bad Agent Design** (too generic):
- Agent 1: Get social data
- Agent 2: Analyze posts
- Agent 3: Send alerts
 
**Why It's Bad**: Not specific enough. What exactly does each agent do?
 
**Good Agent Design** (specific jobs):
 
**Agent 1: Social Data Collection Agent**
- **Specific Job**: Fetch posts from 5 competitors across Twitter and LinkedIn
- **Input**: List of competitor handles/company pages, time range (last 24 hours)
- **Processing**: Authenticate with APIs, paginate through posts, handle rate limits
- **Output**: Standardized JSON array with post text, timestamp, engagement metrics, platform
- **Why Needed**: Different platforms have different API structures; this normalizes data
 
**Agent 2: Trend Detection Agent**
- **Specific Job**: Identify posts with unusual engagement or trending keywords
- **Input**: Standardized posts + baseline engagement data + industry keyword list
- **Processing**: Calculate engagement rate vs. competitor's average, match against trending keywords, score relevance
- **Output**: Posts scored for strategic importance (0-100), with reasoning
- **Why Needed**: Not all posts matter; this filters signal from noise
 
**Agent 3: Digest & Alert Agent**
- **Specific Job**: Generate daily summary and send immediate alerts for high-priority posts
- **Input**: Scored posts from Trend Detection Agent
- **Processing**: Format top 10 posts into digest email, send Slack alerts for >90 importance scores
- **Output**: HTML email digest, Slack messages for urgent items
- **Why Needed**: Humans need digestible formats, not raw JSON
 
---
 
**Example Project 2: Multi-Platform Content Adapter**
 
**Agent 1: Content Extraction Agent**
- **Specific Job**: Pull blog post content and extract key information
- **Input**: Blog post URL
- **Processing**: Fetch HTML, remove boilerplate (header, footer, sidebar), extract title, main points, key quotes
- **Output**: Clean text with structure (title, intro, 3-5 main points, conclusion)
- **Why Needed**: Blog posts have lots of HTML noise; agents need clean content
 
**Agent 2: Platform Adaptation Agent**
- **Specific Job**: Generate platform-specific versions of the content
- **Input**: Clean structured content + platform specifications (character limits, best practices, format)
- **Processing**: Use GPT-4 with platform-specific prompts to create Twitter thread, LinkedIn post, Instagram caption
- **Output**: Platform-optimized versions maintaining core message
- **Why Needed**: Each platform has different norms, limits, and audience expectations
 
**Agent 3: Quality Check Agent**
- **Specific Job**: Verify all versions maintain message fidelity
- **Input**: Original content + all adapted versions
- **Processing**: Check that key points appear in all versions, verify tone consistency, flag any major deviations
- **Output**: Quality score per version + flagged issues
- **Why Needed**: Automated generation can drift from original message; this catches problems
 
---
 
**Example Project 3: Synthetic Persona Generator**
 
**Agent 1: Data Ingestion Agent**
- **Specific Job**: Process customer data from multiple sources
- **Input**: Survey CSV, interview transcripts (text files), customer support tickets
- **Processing**: Parse structured data (CSV), extract text from unstructured sources, normalize into common format
- **Output**: Array of customer responses with metadata (source, date, customer ID)
- **Why Needed**: Data comes in different formats; needs standardization
 
**Agent 2: Pattern Detection Agent**
- **Specific Job**: Identify common themes in goals, pain points, behaviors
- **Input**: Customer response array
- **Processing**: Use NLP to extract mentioned goals, pain points, preferences; cluster similar patterns
- **Output**: Theme clusters with frequency counts and example quotes
- **Why Needed**: Personas are based on patterns, not individual responses
 
**Agent 3: Persona Synthesis Agent**
- **Specific Job**: Generate readable persona documents from pattern data
- **Input**: Theme clusters + demographic data (if available)
- **Processing**: Use GPT-4 to synthesize patterns into persona narratives (demographics, goals, pain points, behaviors, preferences)
- **Output**: 3-5 persona documents in standard format
- **Why Needed**: Data patterns need to become human-readable personas marketers can use
 
---
 
### Agent Communication
 
**How Do They Talk to Each Other?**
 
Agents pass data through a defined structure.
 
**Competitor Monitor Flow**:  
User Input (Competitor List) -> [Social Data Collection Agent] -> Standardized Posts JSON -> [Trend Detection Agent] -> Scored Posts -> [Digest & Alert Agent] -> Email + Slack Notifications
 
**Content Adapter Flow**:  
User Input (Blog URL) -> [Content Extraction Agent] -> Clean Structured Content -> [Platform Adaptation Agent] -> Platform-Specific Versions -> [Quality Check Agent] -> Reviewed Versions + Quality Scores
 
**Synthetic Persona Flow**:  
User Input (Customer Data Files) -> [Data Ingestion Agent] -> Normalized Customer Responses -> [Pattern Detection Agent] -> Theme Clusters -> [Persona Synthesis Agent] -> Persona Documents
 
---
 
### The MVP Scope (15 points)
 
**MVP = Minimum Viable Product**
 
The simplest version that proves your concept works.
 
**The Hard Truth**: You don't have time to build everything.
 
**The Goal**: Build the ONE workflow that proves your core idea.
 
### Define Your ONE Workflow
 
**What to Include**:
 
**1. The ONE workflow you'll create in n8n**
 
Be specific about what it does.
 
**Competitor Monitor**: "A workflow that: takes a list of 5 competitor Twitter handles, fetches their last 24 hours of posts, scores them for strategic importance, and emails a digest of the top 10 posts"
 
**Content Adapter**: "A workflow that: takes a blog post URL, extracts the content, generates Twitter thread + LinkedIn post + Instagram caption, and outputs all three in a formatted document"
 
**Synthetic Personas**: "A workflow that: takes a CSV of survey responses (100 rows), identifies 3-4 customer segments, and generates persona documents for each segment"
 
**2. The 3-5 nodes you'll use**
 
n8n works with "nodes" (building blocks). Name them specifically.
 
**Common Nodes**:
- HTTP Request (fetch web content, call APIs)
- OpenAI (run GPT analysis)
- Function (custom JavaScript)
- Code (Python scripts)
- Switch (conditional logic)
- Set (modify data)
- JSON (parse/format JSON)
- Email (send notifications)
- Slack (send messages)
 
**Competitor Monitor Example**:  
Node 1: HTTP Request - Fetch tweets via Twitter API Node 2: Function - Parse JSON, extract text and metrics Node 3: Function - Calculate engagement rate vs. baseline Node 4: OpenAI - Analyze for trending industry keywords Node 5: Email - Send daily digest
 
**Content Adapter Example**:  
Node 1: HTTP Request - Fetch blog post HTML Node 2: Function - Extract clean text, remove HTML Node 3: OpenAI - Generate Twitter thread version Node 4: OpenAI - Generate LinkedIn post version Node 5: OpenAI - Generate Instagram caption version
 
**3. The input → process → output flow**
 
Map it out clearly.
 
**Competitor Monitor Example**:  
INPUT:  
List of 5 competitor Twitter handles  
Time range: last 24 hours

PROCESS:  
For each competitor, fetch tweets from Twitter API  
Parse JSON responses, extract text, likes, retweets  
Calculate engagement rate (likes + retweets) / followers  
Flag posts with 2x+ average engagement  
Send flagged posts to GPT-4 for keyword matching  
Format top 10 as email digest with links

OUTPUT:  
Daily email with top 10 competitor posts  
Each post shows: text snippet, engagement metrics, why it was flagged  
Links to original posts
 
**Content Adapter Example**:  
INPUT:  
Blog post URL  
Target platforms: Twitter, LinkedIn, Instagram  

PROCESS:  
Fetch HTML from URL  
Extract text, remove HTML/CSS/JS  
Identify title, main points (3-5), conclusion  
Send to GPT-4 with Twitter prompt (create thread, hook-first, <280 chars per tweet)  
Send to GPT-4 with LinkedIn prompt (professional tone, business context, 1300 chars)  
Send to GPT-4 with Instagram prompt (visual language, hashtags, 2200 chars)  
Format all versions in document

OUTPUT:  
Twitter thread (5-7 tweets)  
LinkedIn post (professional, ready to paste)  
Instagram caption (with 5-10 hashtags)
 
**4. What you WON'T build**
 
**This is critical.** Being clear about what's out of scope keeps you focused.
 
**Competitor Monitor - NOT IN MVP**:  
NOT INCLUDED IN MVP:  
❌ LinkedIn monitoring (just Twitter for MVP)  
❌ Historical trend analysis (just 24-hour snapshots)  
❌ Sentiment analysis (just engagement metrics)  
❌ Automated posting responses (just monitoring)  
❌ Visual dashboard (just email digest)  
❌ Customizable filters (fixed importance algorithm)  
WHY: Core concept is "can we automate competitor monitoring?" Everything else is polish. Can add in Week 4.
 
**Content Adapter - NOT IN MVP**:  
NOT INCLUDED IN MVP:  
❌ Additional platforms (just Twitter/LinkedIn/Instagram)  
❌ Image generation (text only)  
❌ Automated posting (adaptation only, not distribution)  
❌ Performance tracking (creation only, not analytics)  
❌ Content calendar integration (standalone tool)  
❌ Team collaboration (single user)  
WHY: Proving we can maintain message fidelity while adapting to platform norms. Distribution comes later.
 
**Synthetic Personas - NOT IN MVP**:  
NOT INCLUDED IN MVP:  
❌ Real-time data ingestion (batch processing only)  
❌ Visual persona cards (text documents only)  
❌ Journey mapping (just personas)  
❌ Competitive persona comparison (single company focus)  
❌ Persona updates over time (snapshot only)  
❌ Integration with CRM/marketing platforms  
WHY: Core concept is "can we generate data-driven personas from customer data?" Integrations come later.
 
**Why This Matters**:
- Prevents scope creep
- Sets realistic expectations
- Shows strategic thinking
- Keeps you from burning out
 
### Realistic Scope Check
 
**Can you build this in Week 3?**
 
Ask yourself:
- Do I understand what each component does?
- Have I seen examples of similar workflows?
- Are the APIs available and documented?
- Is the processing complexity reasonable?
- Can I test this incrementally?
 
**Red Flags**:
- "I'll figure out the hard parts as I go"
- "I'll probably need custom AI training"
- "This requires data I don't have access to"
- "I've never used any of these technologies"
 
**Green Flags**:
- "I've seen similar n8n examples I can adapt"
- "The APIs have good documentation"
- "I can test each step independently"
- "If GPT-4 doesn't work, I have a Plan B"
 
---
 
### Excellence Level (Technical Architecture)
 
**Get 6 Additional Points By**:
 
**1. Specific n8n Nodes Researched (3 pts)**
 
Don't just name nodes. Show you've researched them.
 
**Example**:  
HTTP Request Node (Twitter API):  
Using Twitter API v2 timeline endpoint  
Bearer token authentication  
Pagination to get 50 tweets (25 per request max)  
Rate limit: 75 requests per 15 min window  
Will implement exponential backoff for rate limit errors

Function Node (Engagement Calculation):  
JavaScript to calculate (likes + retweets) / follower_count  
Handle edge case: division by zero if new account  
Compare to 30-day rolling average per competitor  
Return anomaly score: current / average

OpenAI Node (Trend Analysis):  
GPT-4-turbo model  
Temperature: 0.2 (need consistent analysis)  
System prompt includes industry keyword list  
Returns JSON with matched keywords and relevance score
 
**2. Data Schemas Shown (3 pts)**
 
Define the exact data structure between agents.
 
**Competitor Monitor Example**:
```javascript
// Output from Social Data Collection Agent
{
  "competitor": "string (Twitter handle)",
  "posts": [
	{
  	"id": "string",
  	"text": "string",
  	"timestamp": "ISO-8601 datetime",
  	"metrics": {
    	"likes": "integer",
    	"retweets": "integer",
    	"replies": "integer"
  	},
  	"url": "string"
	}
  ],
  "competitor_baseline": {
	"avg_engagement_rate": "float",
	"follower_count": "integer"
  }
}
 
// Output from Trend Detection Agent
{
  "post_id": "string",
  "strategic_score": "integer (0-100)",
  "flags": {
	"engagement_anomaly": "boolean",
	"trending_keywords": ["array of strings"],
	"sentiment_shift": "boolean"
  },
  "reasoning": "string (why this scored high)"
}
 
// Output from Digest Agent
{
  "digest_date": "ISO-8601 date",
  "high_priority_posts": [
	{
  	"competitor": "string",
  	"text_snippet": "string (first 100 chars)",
  	"score": "integer",
  	"why_important": "string",
  	"url": "string"
	}
  ],
  "alerts_sent": "integer"
}
```

**Content Adapter Example**:

```javascript
// Output from Content Extraction Agent
{
  "source_url": "string",
  "extracted_content": {
	"title": "string",
	"intro": "string",
	"main_points": ["array of strings"],
	"conclusion": "string",
	"word_count": "integer"
  },
  "extraction_quality": "float (0-1)"
}
 
// Output from Platform Adaptation Agent
{
  "original_url": "string",
  "adaptations": {
	"twitter": {
  	"format": "thread",
  	"tweets": ["array of strings"],
  	"total_chars": "integer",
  	"hashtags": ["array of strings"]
	},
	"linkedin": {
  	"format": "single_post",
  	"text": "string",
  	"char_count": "integer",
  	"includes_link": "boolean"
	},
	"instagram": {
  	"format": "caption",
  	"text": "string",
  	"hashtags": ["array of strings"],
  	"emoji_count": "integer"
	}
  }
}
 
// Output from Quality Check Agent
{
  "original_message": "string (core message)",
  "version_scores": {
	"twitter": {
  	"fidelity_score": "integer (0-100)",
  	"issues": ["array of strings or empty"]
	},
	"linkedin": {
  	"fidelity_score": "integer (0-100)",
  	"issues": ["array of strings or empty"]
	},
	"instagram": {
  	"fidelity_score": "integer (0-100)",
  	"issues": ["array of strings or empty"]
	}
  },
  "overall_quality": "integer (0-100)"
}
```

**Why This Matters**:
- Shows you think like an engineer
- Makes implementation much easier
- Helps others understand your architecture
- Demonstrates technical depth

---

## Part 5: Peer Review - Help Your Teammate Succeed

### Why Peer Review Matters

**The Reality**: The best engineers give and receive feedback constantly.

**In This Course**: Peer review isn't optional. It's how you:
- Catch problems before you build
- Learn from others' approaches
- Practice professional communication
- Build your network

*Not doing peer review = -2 points. Don't skip this.*

### Finding Your Review Partner

**Timeline**:
- By September 24: Connect with ONE person on your team
- By September 26: Complete reviews

**How to Connect**:
- Check Canvas for your assigned team
- Reach out via Slack or email
- Exchange Figma board links
- Schedule time to review (don't wait until last minute)

### What to Review

Leave THREE comments on their Figma board:

#### 1. One Strength

What's working well in their PRD or architecture?

❌ **Generic**: "Looks good!"  
✅ **Specific**: "Your problem statement is strong—the quantified time savings (10-15 hours/week) immediately shows business value. The HubSpot research citation adds credibility."

#### 2. One Technical Suggestion

How could their agent design or n8n flow improve?  

❌ **Vague**: "Maybe add more features?"  
✅ **Specific**: "Your agent architecture could benefit from error handling. What happens if the Twitter API rate limit hits? Consider adding a retry queue agent that stores failed requests and processes them when the rate limit resets."

**Another Example**: "Your Content Adapter has three separate OpenAI calls (one per platform). That's expensive and slow. Consider batching—send one request asking for all three versions in a single JSON response. Cuts cost by 66% and makes it 3x faster."

#### 3. One Scope Check

Is their Week 3 build realistic?

❌ **Generic**: "Seems fine"  
✅ **Honest**: "You have 5 different API integrations in your MVP (Twitter, LinkedIn, Instagram, Facebook, TikTok). That's ambitious for Week 3. Consider starting with just Twitter + LinkedIn (easiest APIs, best documentation), then adding the others in Assignment 4 after you have the core working."

**Another Example**: "Your synthetic persona generator plans to process interview transcripts, surveys, AND support tickets. For Week 3 MVP, pick ONE data source (surveys are easiest—structured data). Add the others in Week 4 once you've proven the concept."

### Comment Format

Start with your name:

`"[Your Name] - Your agent architecture is clear, but consider batching the OpenAI calls to reduce cost and latency."`

**Why**: Makes it easy to track who said what, professional habit.

### Completion Proof

**Required**: Screenshot your comments on their board.
**Include in Canvas submission**: Show you actually did this.

**What to capture**:
- Your comments visible
- Your name in comments
- The Figma board they're on
- Timestamp/date

## Your Complete Figma Board

### All Sections Required

**1. Dream Job**
- Company + exact title
- Link to posting or screenshot
- Top 3 technical requirements
- Why this role (one sentence)
- [Excellence: Hiring manager research]

**2. Gap Analysis Table**
- At least 3 rows
- All four columns filled
- Specific gaps and solutions
- Clear Madison connections
- [Excellence: Tech stack research]

**3. PRD - Problem Statement**
- Specific problem
- Who experiences it
- Cost of not solving
- Quantified impact

**4. PRD - Proposed Solution**
- Your Madison approach
- Why Madison vs. alternatives
- What's technically interesting
- Clear technical decisions

**5. PRD - User Stories**
- Three stories in correct format
- Different user perspectives
- Specific and achievable
- Clear benefits

**6. PRD - Success Metrics**
- What improves + by how much
- Time/errors/cost calculations
- How you'll measure
- Testing methodology

**7. Technical Architecture - Agents**
- 3+ agents with distinct roles
- Specific job for each
- Communication flow diagram
- Data passing explained
- [Excellence: Data schemas]

**8. Technical Architecture - MVP Scope**
- One specific workflow
- 3-5 n8n nodes listed
- Input → process → output flow
- What you WON'T build (critical!)
- [Excellence: Node research details]

**9. Peer Review Screenshots**
- Your comments on teammate's board
- Your name visible
- Three comments (strength, technical, scope)
- Dated before deadline

## Grading Rubric Reference

**Part 1: Dream Job (10 points)**
- Complete (8): Real job, link, requirements, rationale
- Excellence (+2): Hiring manager research

**Part 2: Gap Analysis (20 points)**
- Complete (16): 3+ rows, specific gaps, Madison solutions
- Excellence (+4): Tech stack research

**Part 3: PRD (40 points)**
- Problem Statement (8): Specific, quantified, impactful
- Proposed Solution (8): Clear approach, technical depth
- User Stories (8): 3 stories, proper format
- Success Metrics (8): Measurable, testable
- Excellence (+8): Industry research, API references, edge cases, professional format

**Part 4: Technical Architecture (30 points)**
- Agent Design (12): 3+ agents, clear roles, communication
- MVP Scope (12): One workflow, realistic, what's excluded
- Excellence (+6): Node research (3), data schemas (3)

**Total: 100 points maximum**

## Common Pitfalls & How to Avoid Them

**Pitfall #1: Vague Problem Statement**
- **Symptom**: "Marketing is hard"
- **Fix**: Get specific. Who? What exactly? How much does it cost?

**Pitfall #2: Unrealistic Scope**
- **Symptom**: "I'll integrate with 10 different platforms"
- **Fix**: ONE workflow that proves ONE concept. Add more later.

**Pitfall #3: Generic User Stories**
- **Symptom**: "As a user, I want it to work"
- **Fix**: Specific role, specific feature, specific benefit

**Pitfall #4: No Success Metrics**
- **Symptom**: "I'll know it works when it's better"
- **Fix**: Define "better" with numbers. 85% accuracy? 10x faster?

**Pitfall #5: Too Technical for Business**
- **Symptom**: PRD reads like an engineering spec
- **Fix**: Lead with business value, support with technical details

**Pitfall #6: Agents Doing Everything**
- **Symptom**: "Agent 1 handles all the work"
- **Fix**: Break into smaller, specific agents with clear jobs

**Pitfall #7: No Error Handling**
- **Symptom**: Architecture assumes everything works perfectly
- **Fix**: What happens when APIs fail? Timeouts occur? Bad data comes in?

**Pitfall #8: Copied Examples**
- **Symptom**: Your PRD sounds exactly like another student's
- **Fix**: This is YOUR project. Your problem. Your solution. Make it specific to you.

**Pitfall #9: Building What's Already Built**
- **Symptom**: "I'll build a social media scheduler like Hootsuite"
- **Fix**: Don't recreate existing tools. Extend them, integrate them, or solve adjacent problems they don't address.

**Pitfall #10: Solving Imaginary Problems**
- **Symptom**: "I think marketing teams might need this"
- **Fix**: Talk to actual marketers. Read marketing subreddits. Understand real pain points.

## Pro Tips from the Instructors

### From Nina (40 years in brand):

> "The PRD is your creative brief for engineering. If you can't explain why you're building something in two sentences, you don't understand the problem well enough. Start with the 'why'—the technical 'how' is easier once you're clear on the problem."

> "I've hired dozens of marketing technologists. The ones who get offers? They understand the marketing problem deeply and can explain their technical solution simply. Not the reverse."

### From Nik (AI engineering):

> "Most students make their MVP too complex. You don't need 10 features to prove your concept. You need ONE thing that works really well. Build that first. The GitHub history of successful projects shows they all started simple and iterated. Start simple."

> "Don't build every Madison project as an archetype detector. We'll cover archetypes deeply in Week 7 for YOUR brand strategy. Your Madison project should solve whatever marketing problem interests you—social monitoring, content generation, data analysis, automation, whatever. Archetypes are one option among many."

**On Scope**:
> "I've seen brilliant students fail because they tried to build everything at once. And I've seen average students succeed because they shipped something simple that worked, then improved it. Be the student who ships."

**On Peer Review**:
> "The feedback you give your classmate is practice for code reviews, which you'll do every single day as a professional. Learn to give specific, constructive feedback now. 'Looks good' helps no one. 'Your API error handling could use exponential backoff' helps everyone."

**On Problem Selection**:
> "Build tools that solve problems you've experienced or witnessed firsthand. If you've worked in marketing, what frustrated you? If you haven't, interview someone who has. The best products come from deep problem understanding, not surface-level assumptions."

## Real Student Examples

### Example 1: Strong PRD Structure - Social Media Monitor

**Problem Statement**:

> "Social media managers at B2B SaaS companies (Series A-B, 20-100 employees) face an impossible monitoring challenge. They need to track 5-10 competitors across 3-4 platforms to identify trending topics, competitive moves, and engagement patterns—but manual checking takes 10-15 hours per week.
>
> At $60-75K salaries, that's $15,000-23,000 annually just spent on manual scrolling. Worse, manual monitoring happens during work hours, missing 60-70% of competitor posts published evenings/weekends. When a competitor's post goes viral overnight, it's too late to join the conversation by Monday morning.
>
> The cost: missed thought leadership opportunities, slower competitive response, and appearing less informed than competitors who've automated this. Per Sprout Social's research, brands responding to trending topics within 2 hours see 3x engagement vs. 24-hour responses."

**Why It's Strong**:
- Specific company type and size
- Specific role and pain point
- Quantified time (10-15 hours/week)
- Calculated cost ($15-23K annually)
- Competitive disadvantage clearly stated
- Cited research (Sprout Social)

### Example 2: Strong PRD Structure - Content Adapter

**Problem Statement**:

> "Content marketing teams at small-medium businesses face a distribution bottleneck. After investing 8-10 hours creating a comprehensive blog post, they need to manually reformat it for 5-6 platforms. Each platform requires different: character limits (Twitter 280, LinkedIn 3000), tone adjustments (professional vs. casual), formatting (threads vs. single posts), and visual elements (hashtags, emojis).
>
> Manual adaptation takes 30-45 minutes per platform. For teams publishing 4 blog posts monthly across 5 platforms, that's 10-15 hours monthly spent reformatting—120-180 hours annually at $40-50/hour = $4,800-9,000 in labor costs.
>
> The real cost: teams skip platforms due to time constraints. Great content reaches 2 platforms instead of 6, reducing potential reach by 60-70%. Per Content Marketing Institute, multi-platform distribution increases content ROI by 3-5x, but most small teams can't afford the time."

**Why It's Strong**:
- Specific pain point (distribution bottleneck)
- Quantified adaptation time
- Calculated annual cost
- Opportunity cost clearly stated
- Industry research cited

### Example 3: Clear Agent Architecture - Social Monitor

**Project**: Competitor Social Intelligence System

**Agent 1: Multi-Platform Scraper Agent**
- **Job**: Fetch competitor posts from Twitter and LinkedIn
- **Input**: Competitor handles/pages, fetch timeframe (24 hours)
- **Processing**: Twitter API v2 for tweets, LinkedIn API for company posts, handle pagination and rate limits
- **Output**: Normalized JSON array with posts, metrics, timestamps across both platforms
- **Why**: Different API structures need normalization before analysis

**Agent 2: Strategic Importance Scorer Agent**
- **Job**: Identify which posts matter strategically vs. routine updates
- **Input**: Normalized posts + competitor baseline engagement data + industry keyword list
- **Processing**: Calculate engagement anomalies, match against strategic keywords (product, pricing, hiring, partnership), score each post 0-100
- **Output**: Posts with strategic importance scores and reasoning
- **Why**: Reduces 100+ daily posts to 10-15 strategic ones worth reviewing

**Agent 3: Notification & Digest Agent**
- **Job**: Alert on high-priority posts immediately, send daily digest of top posts
- **Input**: Scored posts from Agent 2
- **Processing**: Posts scored >90 = immediate Slack alert, all posts >70 = daily email digest, format with context and links
- **Output**: Real-time Slack messages for urgent items, HTML email for daily review
- **Why**: Different urgency levels need different notification methods

**Why This Works**:
- Each agent has ONE clear, specific job
- Inputs and outputs are precisely defined
- Reasoning explains architectural decisions
- Communication flow is clear
- Realistic scope for 3-week build

### Example 4: Realistic MVP Scope - Content Adapter

**MVP (Week 3)**:
- Single workflow: Blog URL → 3 platform versions (Twitter, LinkedIn, Instagram)
- **Input**: Blog post URL
- **Process**: Extract content, generate 3 adaptations using GPT-4
- **Output**: Text document with all three versions ready to copy/paste
- **Timeline**: Can build and test in 5-6 days

**NOT IN MVP** (can add in Week 4):
- ❌ Additional platforms (YouTube, Facebook, TikTok)
- ❌ Image generation for each platform
- ❌ Automated posting/scheduling
- ❌ Performance tracking
- ❌ Content calendar integration
- ❌ Team approval workflow
- ❌ Historical content library

**Week 4 Potential Additions**:
- Add 2 more platforms
- Add basic image suggestions (text prompts for DALL-E)
- Add quality scoring
- Add simple web UI (replace text document output)

**Why This Works**:
- MVP proves core concept ("can we maintain message while adapting to platforms?")
- Week 4 adds polish/features based on testing
- Clear about what's excluded
- Realistic timeline
- Shows strategic thinking about incremental delivery

## Tools & Resources

### For PRD Writing
- **Templates**: Google "PRD template" for examples from real companies (Atlassian, Intercom have public ones)
- **Examples**: Product Hunt discussions often include PRD links
- **AI Help**: "Help me write a problem statement for a competitor monitoring tool"

### For Gap Analysis
- **Job Search**: LinkedIn Jobs (best for tech roles), AngelList (startups)
- **Salary Research**: Levels.fyi, Glassdoor
- **Tech Stack Research**: StackShare, company engineering blogs, GitHub org pages

### For Agent Architecture
- **Madison Repo**: https://github.com/Humanitariansai/Madison - study existing components
- **n8n Examples**: n8n.io/workflows (community-contributed workflows)
- **Multi-Agent Patterns**: Search "multi-agent system architecture" for design patterns

### For Marketing Problem Research
- **/r/marketing on Reddit**: Real marketers discussing real problems
- **Marketing automation forums**: See what people struggle with
- **Marketing tool review sites**: G2, Capterra reviews show pain points
- **Marketing podcasts/blogs**: Listen to what practitioners complain about

### For Scope Planning
- **Reality Check**: Talk to Nik or Nina in office hours
- **Estimation**: Look at similar n8n workflows, estimate 2x what you think
- **Backup Plan**: Always have a simpler version if things break

## Quick Start Checklist

### Dream Job Analysis
- [ ] Found real job posting (6-12 months achievable)
- [ ] Copied exact title and company name
- [ ] Saved link or screenshot
- [ ] Listed top 3 technical requirements
- [ ] Wrote one sentence on why this role
- [ ] [Optional] Researched hiring manager

### Gap Analysis
- [ ] Created 4-column table on Figma
- [ ] Listed at least 3 gaps
- [ ] Honest about current skill level
- [ ] Specific about what's missing
- [ ] Connected Madison project to gap
- [ ] [Optional] Researched company tech stack

### PRD - Problem Statement
- [ ] Identified specific problem
- [ ] Named who experiences it
- [ ] Quantified the cost
- [ ] Cited sources if possible
- [ ] Made it compelling

### PRD - Proposed Solution
- [ ] Explained Madison approach clearly
- [ ] Compared to alternatives
- [ ] Discussed technical challenges
- [ ] Showed why your approach works

### PRD - User Stories
- [ ] Wrote 3 user stories
- [ ] Used correct format
- [ ] Specific roles, features, benefits
- [ ] Realistic and achievable

### PRD - Success Metrics
- [ ] Defined what improves
- [ ] Quantified improvements
- [ ] Specified measurement method
- [ ] Included testing plan

### Technical Architecture - Agents
- [ ] Designed 3+ agents
- [ ] Gave each specific job
- [ ] Defined inputs and outputs
- [ ] Drew communication diagram
- [ ] [Optional] Defined data schemas

### Technical Architecture - MVP
- [ ] Defined ONE workflow
- [ ] Listed 3-5 n8n nodes
- [ ] Mapped input → process → output
- [ ] Listed what's NOT included
- [ ] Ensured scope is realistic for Week 3
- [ ] [Optional] Researched nodes in detail

### Peer Review
- [ ] Connected with ONE teammate
- [ ] Reviewed their Figma board
- [ ] Left strength comment
- [ ] Left technical suggestion
- [ ] Left scope check comment
- [ ] Screenshot comments
- [ ] Included in submission

### Final Quality Check
- [ ] All sections on Figma board
- [ ] Clear organization
- [ ] Professional presentation
- [ ] No spelling errors
- [ ] Shareable link created
- [ ] Link tested

**Time Investment**: 8-10 hours  
**What You'll Have**: A PRD you could show at a job interview  
**What You'll Know**: How to plan like a product manager

---

← Previous: [Foundation Setup](../Module%201/README.md) | Next: Data Pipeline →

*The students who plan well build fast. The students who skip planning rebuild three times.*
 
 
